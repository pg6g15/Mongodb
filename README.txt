Introduction:1.Import the csv file to the local database	1)Create a database named ‘TWEETS’ and a collection named ‘tweets’	2)Import the csv to database TWEETS’, collection named ‘tweets’		mongoimport --db TWEETS --collection tweets --type csv --headerline --file 		‘CSV file location‘2.Run the file	1)Run the file “mongo_1“. 		Here use distinct function to find the how many “id_member“ in database 		“db.tweets“	2)Run the file “mongo_2“. 		Use mapreduce function first. Group the data which has the same 			“id_member“ and return how many the same “id_member“ to “value” and store 		the data to a new collection “top10_tweets“. Next use aggregate function. 		The pipeline is to sort the “value“ in “top10_tweets“ first and give limit 		10. It will show the top 10 users’ tweets. 	3)Run the file “mongo_3“. 		Use aggregate function to sort “timestamp“ data in collection “tweets“. 		However, to sort the data in ascending way, it needs to clean data first. 		Here use “$match“ if “timestamp“ is not null		It shows the earliest and latest data of tweets	4)Run the file “mongo_4“.		Use aggregate function to sort “timestamp“ data in collection “tweets“. 		And store the string to a list in python. Separate the string and 			calculate the time delta between all messages.	5)Run the file “mongo_5“.		Here use mapreduce to simplify the data. Only show the “_id“ and the 			length of “text“. Store the data to a new collection “mean_length“ Use 			aggregate to extract all data in “value“ which is the length of text. Add 		the data and calculate the mean length of a message	6)Run the file “mongo_6“.		Only calculate the unigram. Extract all “text“ in collection “tweets“ and 		separate the string to several parts. There is a “total_word” list which 		is used to store all words. If the separated string is not in 		“total_word“, add it to “total_word“. The list “total_number“ corresponds 		to “total_word“, which stores who many same words. Find the top 10 number 		and check back what the word is. Give a limit to test the program and it 		works correctly. And it will cost lots of time to test all data.	7)Run the file “mongo_7“.		Use mapreduce to find if “#“ is in “text“. First, check “text“ whether a 		string or not. If not, give it “[ ]“ to clean the data. The next step is 		to counting the number “#“ and store the result. Give the number of 			average “#” in tweets.	8)Run the file “mongo_8“.		Use mapreduce to aggregate “geo_lat“ and “geo_lng” together. And store the 		value to collection “lat_lng“. The “geo_lat“ and “geo_lng” are simplified 		here. Only store two decimals. Cause to find the area within UK contains 		the largest number of published message. It needs to extend the range. For 		two decimals, it can give the place in a city. Here, the answer is near 		the Hyde Park, which has the largest number of published message. However, 		the output is the geographic latitude and longitude coordinates. To find 		the place, please use google map. 